{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpJ3M5c_0ok"
   },
   "source": [
    "# **Chatbot**\n",
    "\n",
    "We're here to help answer all your questions and guide you on your learning journey.\n",
    "\n",
    "Our chatbot is equipped with a vast knowledge base and is designed to provide accurate and reliable information. We understand that every student has unique needs, so feel free to ask any questions, no matter how big or small. We're committed to supporting your academic growth and ensuring you have a seamless learning experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RM4fWlQ_2m5",
    "outputId": "26c3d20b-cd61-457c-d453-58cee5a106f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the necessary resources from the Natural Language Toolkit (NLTK) library\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amKrMx9ZCbPb"
   },
   "source": [
    "### **Importing Neccessary libraries and modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intel optimizations\n",
    "`from tensorflow import keras` Imports the Keras module from TensorFlow, a high-level neural networks API  \n",
    "`from tensorflow.keras import mixed_precision` Imports the mixed-precision module from Keras for mixed-precision training.  \n",
    "`policy = mixed_precision.Policy('mixed_bfloat16')` Sets the mixed-precision policy to use mixed bfloat16 data type.   \n",
    "`mixed_precision.set_global_policy(policy)` Applies the mixed-precision policy globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m22jAGgaBnBz"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_bfloat16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRzBvnD7C_iK"
   },
   "outputs": [],
   "source": [
    "# The variables 'words', 'classes', 'documents', and 'ignore_words' are essential components used during the training process of the chatbot.\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "# Open the 'intents.json' file containing the predefined intents and their corresponding data.\n",
    "data_file = open('intents.json').read()\n",
    "\n",
    "#Read the contents of the file and load it as a JSON object.\n",
    "intents = json.loads(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMKQvyIyFEhi"
   },
   "source": [
    "### **Tokenizing Patterns and Creating Corpus**\n",
    "\n",
    "We are working with a list of intents, where each intent contains a set of patterns. The purpose is to tokenize each word within the patterns and create a corpus for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra0oPCyCEN2R"
   },
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bf2sgTi0Fj_e"
   },
   "outputs": [],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc4WDqHcFyaa"
   },
   "source": [
    "### **Create Training Data**\n",
    "\n",
    "This code takes the patterns and intent tags, tokenizes and lemmatizes the words, and constructs a bag-of-words representation for each pattern.\n",
    "\n",
    "It also prepares the corresponding intent labels in a one-hot encoded format. The resulting training data is shuffled and organized into train_x (patterns) and train_y (intent labels) lists, which are ready for training a model for intent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQDJAlWtFs18"
   },
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in an attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in the current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for the current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pssgda2OGn6-"
   },
   "source": [
    "### **Create Model**\n",
    "This code builds and trains a neural network model for a chatbot, using a three-layer architecture with varying numbers of neurons. The model is compiled with an appropriate optimizer and loss function, and the training process is performed for a specified number of epochs. The resulting model is then saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLFOQcB_Gjs_"
   },
   "outputs": [],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to the number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Fitting and saving the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhVdfkMFHdS8"
   },
   "source": [
    "### **Necessary imports and loading of files for the chatbot model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvaRmD1vHN0_"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('intents.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF7J2ej2fyLN"
   },
   "source": [
    "## **Functions Definitions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PsEK0R7iAle"
   },
   "source": [
    "### **Cleaning up the Sentence**\n",
    "\n",
    "Cleaning up user input before feeding it into the chatbot model. It tokenizes the sentence and applies lemmatization and lowercase conversion to improve matching and understanding during the intent classification and response generation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xkl1ZrONHg0K"
   },
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xrlPfrgkTl2"
   },
   "source": [
    "### **Bag Of Words Function**\n",
    "\n",
    "Converting a sentence into a bag of words representation using the provided vocabulary. The resulting bag of words representation can be used as input for the chatbot model to predict the intent or generate appropriate responses based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNZQRhaefhi4"
   },
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UGs7raRngSK"
   },
   "source": [
    "\n",
    "### **Predicting Intent Class**\n",
    "\n",
    "Enabling the chatbot to predict the intent class for a given input sentence, allowing it to determine the appropriate response based on the predicted intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhB9el00fljg"
   },
   "outputs": [],
   "source": [
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGoW1GG4nvCz"
   },
   "source": [
    "### **Retrieving Response Based on Intents**\n",
    "Fetching an appropriate response based on the predicted intent, providing dynamic and contextually relevant answers to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmJJ57rQflww"
   },
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FPVn7ZDoVzu"
   },
   "source": [
    "\n",
    "### **Chatbot Response Function**\n",
    "Function serves as the core component of the chatbot's conversational logic.\n",
    "\n",
    " It takes user queries as input, predicts the intent class, and retrieves the corresponding response. The function acts as a bridge between the user's input and the trained model, allowing the chatbot to provide relevant and context-specific responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4aQxs7Gfl0O"
   },
   "outputs": [],
   "source": [
    "def chatbot_response(text):\n",
    "    ints = predict_class(text, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqI5Bi1Xo2hr"
   },
   "source": [
    "## **User-Chatbot Interaction Loop**\n",
    " The user can enter their input, and the chatbot will generate a response based on the provided input.\n",
    "\n",
    " The conversation continues until the user explicitly enters 'exit', allowing for an interactive and dynamic chatbot experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNOOG1R5fhsg"
   },
   "outputs": [],
   "source": [
    "user_input = input(\"You: \")\n",
    "while user_input.lower() != 'exit':\n",
    "    response = chatbot_response(user_input)\n",
    "    print(\"Bot:\", response)\n",
    "    user_input = input(\"You: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLlFO7bwfh2C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
