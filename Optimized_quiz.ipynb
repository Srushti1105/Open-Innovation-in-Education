{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G7wIP389FPkB"
   },
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIjcc1u1Fk7I"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet flashtext==2.7  # Install flashtext library version 2.7\n",
    "!pip install git+https://github.com/boudinfl/pke.git  # Install pke library from GitHub repository\n",
    "!pip install --quiet transformers==4.8.1  # Install transformers library version 4.8.1\n",
    "!pip install --quiet sentencepiece==0.1.95  # Install sentencepiece library version 0.1.95\n",
    "!pip install --quiet textwrap3==0.9.2  # Install textwrap3 library version 0.9.2\n",
    "!pip install --quiet strsim==0.0.3  # Install strsim library version 0.0.3\n",
    "!pip install --quiet sense2vec==2.0.0  # Install sense2vec library version 2.0.0\n",
    "!pip install --quiet sentence-transformers==2.2.2  # Install sentence-transformers library version 2.2.2\n",
    "!pip install intel-extension-for-pytorch==2.0.100  # Install Intel extension for PyTorch version 2.0.100\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pHXitoafF4z6"
   },
   "source": [
    "### Import the wrap function from the textwrap3 library and Wrap the text into lines of maximum 150 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MSVX5t1GZ4W"
   },
   "outputs": [],
   "source": [
    "from textwrap3 import wrap\n",
    "text = \"\"\"Elon Musk has shown again he can influence the digital currency market with just his tweets. After saying that his electric vehicle-making company\n",
    "Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve\n",
    "system transaction efficiency. Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin\n",
    "rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  In a recent tweet,\n",
    "Musk put out a statement from Tesla that it was “concerned” about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and\n",
    "transaction, and hence was suspending vehicle purchases using the cryptocurrency.  A day later he again tweeted saying, “To be clear, I strongly\n",
    "believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal”.  It triggered a downward spiral for Bitcoin value but\n",
    "the cryptocurrency has stabilised since.   A number of Twitter users welcomed Musk's statement. One of them said it's time people started realising\n",
    "that Dogecoin “is here to stay” and another referred to Musk's previous assertion that crypto could become the world's future currency.\"\"\"\n",
    "\n",
    "for wrp in wrap(text, 150):  # Wrap the text into lines of maximum 150 characters\n",
    "  print(wrp)  # Print each wrapped line\n",
    "print(\"\\n\")  # Print a new line\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xoX4HehCGhHz"
   },
   "source": [
    "# **Summarization of text with T5**\n",
    "Importing libraries, loading the T5 model and tokenizer, setting the device to CPU, and optimizing the model using the Intel Extension for PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel optimizations\n",
    "`import intel_extension_for_pytorch as ipex` Importing intel extension for pytorch  \n",
    "`summary_model = ipex.optimize(summary_model)` Optimizing the model with intel extension for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bxbVxRKGnqk"
   },
   "outputs": [],
   "source": [
    "import torch  # Import the torch library\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer  # Import T5 model and tokenizer from the transformers library\n",
    "import os  # Import the os module\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Set the CUDA_VISIBLE_DEVICES environment variable to '-1' to disable GPU usage\n",
    "\n",
    "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')  # Load the T5 model for conditional generation\n",
    "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')  # Load the T5 tokenizer\n",
    "device = torch.device(\"cpu\")  # Set the device to CPU\n",
    "summary_model = summary_model.to(device)  # Move the summary model to the CPU device\n",
    "\n",
    "import intel_extension_for_pytorch as ipex  # Import the Intel Extension for PyTorch module\n",
    "summary_model = ipex.optimize(summary_model)  # Optimize the summary model using Intel Extension for PyTorch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1FeyQTOdG8pn"
   },
   "source": [
    "#### The purpose of the code below is to set the random seed for reproducibility. It sets the seed for the random module, numpy module, torch module, and all CUDA devices to ensure consistent random behavior across runs. The set_seed function takes an integer seed value as input and applies it to the various modules. Finally, the function is called with a seed value of 42.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWuqLOliG5B2"
   },
   "outputs": [],
   "source": [
    "import random  # Import the random module\n",
    "import numpy as np  # Import the numpy module\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)  # Set the seed for the random module\n",
    "    np.random.seed(seed)  # Set the seed for the numpy module\n",
    "    torch.manual_seed(seed)  # Set the seed for the torch module\n",
    "    torch.cuda.manual_seed_all(seed)  # Set the seed for all CUDA devices\n",
    "\n",
    "set_seed(42)  # Call the set_seed function with seed value 42\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lPk1iJaCHaE_"
   },
   "source": [
    "### Importing libraries, downloading necessary resources from NLTK, defining functions for text post-processing and summarization, generating a summary, and printing the original and summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kpe5tRGgHafp"
   },
   "outputs": [],
   "source": [
    "import nltk  # Import the nltk library\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer data\n",
    "nltk.download('brown')  # Download the Brown corpus data\n",
    "nltk.download('wordnet')  # Download the WordNet corpus data\n",
    "from nltk.corpus import wordnet as wn  # Import the WordNet module from nltk.corpus\n",
    "from nltk.tokenize import sent_tokenize  # Import the sent_tokenize function from nltk.tokenize\n",
    "\n",
    "def postprocesstext(content):\n",
    "    final = \"\"  # Initialize an empty string for the final processed text\n",
    "    for sent in sent_tokenize(content):  # Iterate over each sentence in the content\n",
    "        sent = sent.capitalize()  # Capitalize the sentence\n",
    "        final = final + \" \" + sent  # Append the capitalized sentence to the final text\n",
    "    return final  # Return the final processed text\n",
    "\n",
    "\n",
    "def summarizer(text, model, tokenizer):\n",
    "    text = text.strip().replace(\"\\n\", \" \")  # Remove leading/trailing spaces and replace newline characters with spaces\n",
    "    text = \"summarize: \" + text  # Prepend \"summarize: \" to the text\n",
    "    max_len = 512  # Set the maximum length for encoding\n",
    "    encoding = tokenizer.encode_plus(text, max_length=max_len, pad_to_max_length=False, truncation=True,\n",
    "                                     return_tensors=\"pt\").to(device)  # Encode the text using the tokenizer\n",
    "\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]  # Retrieve the input IDs and attention mask\n",
    "\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          early_stopping=True,\n",
    "                          num_beams=3,\n",
    "                          num_return_sequences=1,\n",
    "                          no_repeat_ngram_size=2,\n",
    "                          min_length=75,\n",
    "                          max_length=300)  # Generate the summary using the model\n",
    "\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]  # Decode the generated summary\n",
    "    summary = dec[0]  # Retrieve the first generated summary\n",
    "    summary = postprocesstext(summary)  # Apply post-processing to the summary\n",
    "    summary = summary.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "    return summary  # Return the generated summary\n",
    "\n",
    "\n",
    "summarized_text = summarizer(text, summary_model, summary_tokenizer)  # Generate the summary using the provided text and models\n",
    "\n",
    "print(\"\\noriginal Text >>\")\n",
    "for wrp in wrap(text, 150):  # Wrap the original text into lines of maximum 150 characters\n",
    "    print(wrp)  # Print each wrapped line\n",
    "print(\"\\n\")\n",
    "print(\"Summarized Text >>\")\n",
    "for wrp in wrap(summarized_text, 150):  # Wrap the summarized text into lines of maximum 150 characters\n",
    "    print(wrp)  # Print each wrapped line\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E9qCn2XMHbPr"
   },
   "source": [
    "### Answer Span Extraction (Keywords and Noun Phrases)\n",
    "Importing libraries, downloading necessary resources from NLTK, defining the get_nouns_multipartite function for keyphrase extraction using the MultipartiteRank algorithm,\n",
    "and providing an example usage of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bImNqh5xHwoY"
   },
   "outputs": [],
   "source": [
    "import nltk  # Import the nltk library\n",
    "nltk.download('stopwords')  # Download the stopwords data from NLTK\n",
    "from nltk.corpus import stopwords  # Import the stopwords corpus from nltk.corpus\n",
    "import string  # Import the string module for handling punctuation\n",
    "import pke  # Import the pke library for keyphrase extraction\n",
    "import traceback  # Import the traceback module for error handling\n",
    "\n",
    "def get_nouns_multipartite(content):\n",
    "    out = []  # Initialize an empty list for storing keyphrases\n",
    "    try:\n",
    "        extractor = pke.unsupervised.MultipartiteRank()  # Create an instance of MultipartiteRank for keyphrase extraction\n",
    "        extractor.load_document(input=content, language='en')  # Load the content into the extractor with English language\n",
    "\n",
    "        pos = {'PROPN', 'NOUN'}  # Define the part-of-speech tags to consider as candidate keyphrases\n",
    "        stoplist = list(string.punctuation)  # Create a list of punctuation marks\n",
    "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']  # Add additional brackets to the stoplist\n",
    "        stoplist += stopwords.words('english')  # Add the English stopwords to the stoplist\n",
    "\n",
    "        extractor.candidate_selection(pos=pos)  # Select candidate keyphrases using specified POS tags and stoplist\n",
    "\n",
    "        # Build the Multipartite graph and rank candidates using random walk\n",
    "        # Alpha controls the weight adjustment mechanism, and the threshold determines the keyphrase extraction method\n",
    "        extractor.candidate_weighting(alpha=1.1, threshold=0.75, method='average')\n",
    "\n",
    "        keyphrases = extractor.get_n_best(n=15)  # Retrieve the top 15 keyphrases\n",
    "\n",
    "        for val in keyphrases:\n",
    "            out.append(val[0])  # Append each keyphrase to the output list\n",
    "    except:\n",
    "        out = []  # In case of an error, reset the output list to empty\n",
    "        traceback.print_exc()  # Print the traceback information for debugging purposes\n",
    "\n",
    "    return out  # Return the list of extracted keyphrases\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "content = \"This is an example sentence. Keyphrases are important for text analysis.\"\n",
    "keyphrases = get_nouns_multipartite(content)\n",
    "print(keyphrases)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q84mwsXIGOt"
   },
   "source": [
    "### Importing the KeywordProcessor class from flashtext, defining the get_keywords function for extracting important keywords from the original and summarized text, and providing an example usage of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgEHL7UPIFAh"
   },
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor  # Import the KeywordProcessor class from the flashtext library\n",
    "\n",
    "def get_keywords(originaltext, summarytext):\n",
    "    keywords = get_nouns_multipartite(originaltext)  # Extract keywords from the original text using the get_nouns_multipartite function\n",
    "    print(\"keywords unsummarized: \", keywords)  # Print the extracted keywords\n",
    "\n",
    "    keyword_processor = KeywordProcessor()  # Create an instance of the KeywordProcessor\n",
    "    for keyword in keywords:\n",
    "        keyword_processor.add_keyword(keyword)  # Add each keyword to the keyword processor\n",
    "\n",
    "    keywords_found = keyword_processor.extract_keywords(summarytext)  # Extract keywords from the summary text using the keyword processor\n",
    "    keywords_found = list(set(keywords_found))  # Remove duplicate keywords from the extracted keywords\n",
    "    print(\"keywords_found in summarized: \", keywords_found)  # Print the extracted keywords found in the summary\n",
    "\n",
    "    important_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if keyword in keywords_found:  # Check if a keyword from the original text is present in the extracted keywords from the summary\n",
    "            important_keywords.append(keyword)  # Append the important keyword to the list\n",
    "\n",
    "    return important_keywords[:4]  # Return the top 4 important keywords\n",
    "\n",
    "\n",
    "imp_keywords = get_keywords(text, summarized_text)  # Call the get_keywords function with the provided text and summarized text\n",
    "print(imp_keywords)  # Print the important keywords\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sUg3mIfxIR_K"
   },
   "source": [
    "### Question generation with T5\n",
    "Loading the T5 model and tokenizer for question generation, moving the model to the specified device, and optimizing the model using Intel Extension for PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel optimizations\n",
    "`question_model = ipex.optimize(question_model)` Optimizing the model with intel extension for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLr_1zKfIhFd"
   },
   "outputs": [],
   "source": [
    "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')  # Load the T5 model for question generation\n",
    "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')  # Load the tokenizer for the T5 model\n",
    "question_model = question_model.to(device)  # Move the question model to the specified device (CPU or GPU)\n",
    "question_model = ipex.optimize(question_model)  # Optimize the question model using Intel Extension for PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hJHCzjOwInKH"
   },
   "source": [
    "### The get_question function for generating a question given a context and an answer, the loop to print the summarized text, and the loop to generate questions for each important keyword and print them along with the capitalized form of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNh-RBOgI21-"
   },
   "outputs": [],
   "source": [
    "def get_question(context, answer, model, tokenizer):\n",
    "    text = \"context: {} answer: {}\".format(context, answer)  # Format the context and answer into a text string\n",
    "    encoding = tokenizer.encode_plus(text, max_length=384, pad_to_max_length=False, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          early_stopping=True,\n",
    "                          num_beams=5,\n",
    "                          num_return_sequences=1,\n",
    "                          no_repeat_ngram_size=2,\n",
    "                          max_length=72)\n",
    "\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "\n",
    "    Question = dec[0].replace(\"question:\", \"\")  # Extract the generated question from the output\n",
    "    Question = Question.strip()  # Remove leading and trailing whitespace\n",
    "    return Question\n",
    "\n",
    "\n",
    "# Print the summarized text\n",
    "for wrp in wrap(summarized_text, 150):\n",
    "    print(wrp)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate questions for each important keyword\n",
    "for answer in imp_keywords:\n",
    "    ques = get_question(summarized_text, answer, question_model, question_tokenizer)  # Generate a question for the answer using the question model and tokenizer\n",
    "    print(ques)  # Print the generated question\n",
    "    print(answer.capitalize())  # Print the capitalized form of the answer\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kbU0vPJlI69o"
   },
   "source": [
    "### Filter keywords with Maximum marginal Relevance\n",
    "Downloading and extracting the sense2vec model, loading the sentence transformer model, functions for filtering same sense words, calculating similarity scores,generating sense2vec words, performing MMR (Maximal Marginal Relevance) keyword selection, getting WordNet distractors, and the main code to get distractors for a given keyword and sentence using the loaded models and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlSWf_g5JLhB"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Download and extract sense2vec model\n",
    "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
    "!tar -xvf s2v_reddit_2015_md.tar.gz\n",
    "\n",
    "import numpy as np\n",
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk('s2v_old')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the sentence transformer model\n",
    "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "\n",
    "def filter_same_sense_words(original, wordlist):\n",
    "    filtered_words = []\n",
    "    base_sense = original.split('|')[1]\n",
    "\n",
    "    for eachword in wordlist:\n",
    "        if eachword[0].split('|')[1] == base_sense:\n",
    "            filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "def get_highest_similarity_score(wordlist, wrd):\n",
    "    score = []\n",
    "    for each in wordlist:\n",
    "        score.append(normalized_levenshtein.similarity(each.lower(), wrd.lower()))\n",
    "    return max(score)\n",
    "\n",
    "def sense2vec_get_words(word, s2v, topn, question):\n",
    "    output = []\n",
    "    try:\n",
    "        sense = s2v.get_best_sense(word, senses=[\"NOUN\", \"PERSON\", \"PRODUCT\", \"LOC\", \"ORG\", \"EVENT\", \"NORP\", \"WORK OF ART\", \"FAC\", \"GPE\", \"NUM\", \"FACILITY\"])\n",
    "        most_similar = s2v.most_similar(sense, n=topn)\n",
    "        output = filter_same_sense_words(sense, most_similar)\n",
    "    except:\n",
    "        output = []\n",
    "\n",
    "    threshold = 0.6\n",
    "    final = [word]\n",
    "    checklist = question.split()\n",
    "    for x in output:\n",
    "        if get_highest_similarity_score(final, x) < threshold and x not in final and x not in checklist:\n",
    "            final.append(x)\n",
    "\n",
    "    return final[1:]\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "!pip install scikit-learn-intelex\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_distractors_wordnet(word):\n",
    "    distractors = []\n",
    "    try:\n",
    "        syn = wn.synsets(word, 'n')[0]\n",
    "        word = word.lower()\n",
    "        orig_word = word\n",
    "        if len(word.split()) > 0:\n",
    "            word = word.replace(\" \", \"_\")\n",
    "        hypernym = syn.hypernyms()\n",
    "        if len(hypernym) == 0:\n",
    "            return distractors\n",
    "        for item in hypernym[0].hyponyms():\n",
    "            name = item.lemmas()[0].name()\n",
    "            if name == orig_word:\n",
    "                continue\n",
    "            name = name.replace(\"_\", \" \")\n",
    "            name = \" \".join(w.capitalize() for w in name.split())\n",
    "            if name is not None and name not in distractors:\n",
    "                distractors.append(name)\n",
    "    except:\n",
    "        print(\"Wordnet distractors not found\")\n",
    "\n",
    "    return distractors\n",
    "\n",
    "def get_distractors(word, origsentence, sense2vecmodel, sentencemodel, top_n, lambdaval):\n",
    "    distractors = sense2vec_get_words(word, sense2vecmodel, top_n, origsentence)\n",
    "    if len(distractors) == 0:\n",
    "        return distractors\n",
    "    distractors_new = [word.capitalize()]\n",
    "    distractors_new.extend(distractors)\n",
    "\n",
    "    embedding_sentence = origsentence + \" \" + word.capitalize()\n",
    "    keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
    "    distractor_embeddings = sentencemodel.encode(distractors_new)\n",
    "\n",
    "    max_keywords = min(len(distractors_new), 5)\n",
    "    filtered_keywords = mmr(keyword_embedding, distractor_embeddings, distractors_new, max_keywords, lambdaval)\n",
    "\n",
    "    final = [word.capitalize()]\n",
    "    for wrd in filtered_keywords:\n",
    "        if wrd.lower() != word.lower():\n",
    "            final.append(wrd.capitalize())\n",
    "    final = final[1:]\n",
    "\n",
    "    return final\n",
    "\n",
    "sent = \"What cryptocurrency did Musk rarely tweet about?\"\n",
    "keyword = \"Bitcoin\"\n",
    "\n",
    "print(get_distractors(keyword, sent, s2v, sentence_transformer_model, 40, 0.2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OnUhTVRUJOeT"
   },
   "source": [
    "### Finally generating MCQS\n",
    "\n",
    "Overall process of generating questions from the given context:\n",
    "\n",
    "* Summarize the context using the summarizer model.\n",
    "\n",
    "* Extract the keywords from the context and summary.\n",
    "\n",
    "* Iterate over each keyword and generate a question using the question model and\n",
    " tokenizer.\n",
    "\n",
    "* Retrieve distractors for the answer using the get_distractors function.\n",
    " Generate the output string with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm1oeAg8J4GH"
   },
   "outputs": [],
   "source": [
    "context = '''Once upon a time, there lived a rabbit and tortoise. The rabbit could run fast. He was very proud of his speed. While the turtle was slow and consistent.\n",
    "One day that tortoise came to meet him. The tortoise was walking very slow as usual. The rabbit looked and laughed at him.\n",
    "The tortoise asked “what happened?”\n",
    "The rabbit replied, “You walk so slowly! How can you survive like this?”.\n",
    "The turtle listened to everything and felt humiliated by the rabbit’s words.\n",
    "The tortoise replied, “Hey friend! You are very proud of your speed. Let’s have a race and see who is faster”.\n",
    "The rabbit was surprised by the challenge of the tortoise. But he accepted the challenge as he thought it would be a cakewalk for him.\n",
    "So, the tortoise and rabbit started the race. The rabbit was as usual very fast and went far away. While the tortoise was left behind.\n",
    "After a while, the rabbit looked behind.\n",
    "He said to himself, “The slow turtle will take ages to come near me. I should rest a bit”.\n",
    "The rabbit was tired from running fast. The sun was high too. He ate some grass and decided to take a nap.\n",
    "He said to himself, “I am confident; I can win even if the tortoise passes me. I should rest a bit”. With that thought, he slept and lost the track of time.\n",
    "Meanwhile, the slow and steady turtle kept on moving. Although he was tired, he didn’t rest.\n",
    "Sometime later, he passed the rabbit when the rabbit was still sleeping.\n",
    "The rabbit suddenly woke up after sleeping for a long time. He saw that the tortoise was about to cross the finishing line.\n",
    "He started running very fast with his full energy. But it was too late.\n",
    "The slow turtle had already touched the finishing line. He has already won the race.\n",
    "The rabbit was very disappointed with himself while the tortoise was very happy to win the race with his slow speed. He could not believe his eyes. He was shocked by the end results.\n",
    "At last, the tortoise asked the rabbit “Now who is faster”. The rabbit had learned his lesson. He could not utter a word. The tortoise said bye to the rabbit and left that place calmly and happily.'''\n",
    "\n",
    "def generate_question(context):\n",
    "    summary_text = summarizer(context, summary_model, summary_tokenizer)\n",
    "    np = get_keywords(context, summary_text)\n",
    "\n",
    "    output = \"\"\n",
    "    for answer in np:\n",
    "        ques = get_question(summary_text, answer, question_model, question_tokenizer)\n",
    "        distractors = get_distractors(answer.capitalize(), ques, s2v, sentence_transformer_model, 40, 0.2)\n",
    "\n",
    "        output += f\"\\nQuestion: {ques}\\n\"\n",
    "        output += f\"Answer: {answer.capitalize()}\\n\"\n",
    "\n",
    "        if len(distractors) > 0:\n",
    "            output += \"Distractors:\\n\"\n",
    "            for distractor in distractors[:4]:\n",
    "                output += f\"- {distractor}\\n\"\n",
    "\n",
    "    summary = f\"\\nSummary: {summary_text}\"\n",
    "    output += summary\n",
    "    return output\n",
    "\n",
    "print(generate_question(context))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJ321XRgcRS2MMcCczoo+8",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
